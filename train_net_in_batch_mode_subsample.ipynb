{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import glob\n",
    "from misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subrate = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localNetwork():\n",
    "    def __init__(self, n_features, n_actions, lr=None, trainable = False):\n",
    "        self.hp = HP()\n",
    "        #self.default_nl=tf.nn.relu\n",
    "        self.hp.lr = lr\n",
    "        self.next_layer_id = 0\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.theta = {}\n",
    "        self.estimator = self.vanilla_network()\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, n_actions])\n",
    "        if trainable:\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.q_target, logits=self.estimator)\n",
    "            # self.loss = tf.reduce_mean(tf.losses.absolute_difference(self.q_target, self.estimator))\n",
    "            # tf.losses.absolute_difference()\n",
    "            # self.train_op = tf.train.GradientDescentOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            # self.train_op = tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            self.train_op = tf.train.AdamOptimizer(self.hp.lr).minimize(self.loss)\n",
    "        self.sess = None\n",
    "\n",
    "    def get_layer_id(self):\n",
    "        this_layer_id = self.next_layer_id\n",
    "        self.next_layer_id +=1\n",
    "        return this_layer_id\n",
    "\n",
    "    def vanilla_network(self, layer_size = [None]+[81]*1+[ None]):\n",
    "        layer_size[0] = self.n_features\n",
    "        layer_size[-1] = self.n_actions\n",
    "        next_l = self.input_layer() #todo currently the  number of features in the input layer is defined elsewhere\n",
    "        self.observations = next_l\n",
    "        for ll, ll_size  in enumerate(layer_size[1:-1]):\n",
    "            next_l = self.dense_ff_layer(next_l, ll_size)\n",
    "            next_l = tf.nn.dropout(next_l, 0.98)\n",
    "        ll_size=layer_size[-1]\n",
    "        next_l = self.dense_ff_layer(next_l, ll_size, nl= lambda x: x,g=1e-10)\n",
    "        return next_l\n",
    "\n",
    "\n",
    "    def dense_ff_layer(self, previous_layer, output_size, nl=tf.nn.relu, theta = None,g=1.0):\n",
    "        if theta is None:\n",
    "            this_theta = {}\n",
    "            # print(np.float(np.shape(previous_layer)[-1])**0.5)\n",
    "            this_theta['w'] = tf.Variable(\n",
    "                tf.random_normal(shape=[np.shape(previous_layer)[-1].value, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=g*2.0 / np.sqrt(np.shape(previous_layer)[-1].value)))\n",
    "            this_theta['b'] = tf.Variable(\n",
    "                tf.random_normal(shape=[1, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=0.01))\n",
    "        else:\n",
    "            error('explicit theta is still unsupported')\n",
    "        self.theta[self.get_layer_id()] = this_theta\n",
    "        #print(self.get_layer_id())\n",
    "        ff_layer = nl(tf.matmul(previous_layer, this_theta['w']) + this_theta['b'])\n",
    "        return ff_layer\n",
    "\n",
    "    def input_layer(self):\n",
    "        return tf.placeholder(tf.float32, [None, self.n_features])\n",
    "\n",
    "    # def train_step_op(self):\n",
    "    #     return tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "\n",
    "    def assign_param_prep(self,source_nwk): #todo support more elaborated structures than double dictionary\n",
    "        self.assign_param_op = []\n",
    "        for ll in source_nwk.theta.keys():\n",
    "            for this_param in source_nwk.theta[ll]:\n",
    "                self.assign_param_op.append(tf.assign(self.theta[ll][this_param],\n",
    "                                                 source_nwk.theta[ll][this_param]))\n",
    "\n",
    "    def theta_values(self): #todo support more elaborated structures than double dictionary\n",
    "        t = {}\n",
    "        for ll in self.theta.keys():\n",
    "            t[ll] = {}\n",
    "            for this_param in self.theta[ll]:\n",
    "                t[ll][this_param] = self.theta[ll][this_param].eval(self.sess)\n",
    "        return t\n",
    "\n",
    "    def theta_update(self,t): #todo support more elaborated structures than double dictionary\n",
    "        for ll in t.keys():\n",
    "            for this_param in t[ll]:\n",
    "                self.theta[ll][this_param].assign(t[ll][this_param]).op.run(session=self.sess)\n",
    "\n",
    "    def update(self, sess):\n",
    "        sess.run(self.assign_param_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(nwk, observations, q_target):\n",
    "        return sess.run([nwk.train_op,nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "\n",
    "    def calc_loss(nwk, observations, q_target):\n",
    "        return sess.run([nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "    \n",
    "    def a_eval(nwk,observations):\n",
    "        return sess.run(nwk.estimator,\n",
    "                  feed_dict={nwk.observations: observations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,depth):\n",
    "    o=np.zeros([len(a),depth])\n",
    "    o[list(range(len(a))),a]=1\n",
    "    return o\n",
    "\n",
    "def subsample(data,s):\n",
    "    sub=list(range(0,len(data[0]),s))\n",
    "    return [dd[sub] for dd in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = localNetwork( 128, 3, lr=0.0025, trainable = True)\n",
    "# network = localNetwork( 256*4, 3, lr=0.0025, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_observation_action_from_path(path = None, filenames = None, max_file=1e7):\n",
    "    if filenames is None:\n",
    "        filenames = sorted(glob.glob(path))\n",
    "    observation_list=[]\n",
    "    a_list=[]\n",
    "    for cnt, this_file in enumerate(filenames):\n",
    "        if cnt<max_file:\n",
    "            with open(this_file,'rb') as f:\n",
    "                observation_list_s, a_list_s = pickle.load(f)\n",
    "                observation_list += observation_list_s\n",
    "                a_list += a_list_s\n",
    "        else:\n",
    "            break\n",
    "    return observation_list,a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list,a_list = read_observation_action_from_path('offline_learning_data/YYobservation_action_dump_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xobservation_list,Xa_list = read_observation_action_from_path('offline_learning_data/XXobservation_action_dump_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0 1.0967304\n",
      "eval: 0.6908 1.0654695\n",
      "train 100 0.5342269\n",
      "eval: 0.8132 0.45803475\n",
      "train 200 0.43722582\n",
      "eval: 0.8158 0.4261295\n",
      "train 300 0.3868209\n",
      "eval: 0.8184 0.40573344\n",
      "train 400 0.4323538\n",
      "eval: 0.846 0.38229063\n",
      "train 500 0.38983053\n",
      "eval: 0.8482 0.3724771\n",
      "train 600 0.34090889\n",
      "eval: 0.854 0.36231536\n",
      "train 700 0.38302025\n",
      "eval: 0.856 0.3540086\n",
      "train 800 0.32950607\n",
      "eval: 0.861 0.33960122\n",
      "train 900 0.33009207\n",
      "eval: 0.8662 0.3319243\n",
      "train 1000 0.31940037\n",
      "eval: 0.8676 0.32630983\n",
      "train 1100 0.3411628\n",
      "eval: 0.8676 0.32402325\n",
      "train 1200 0.4319547\n",
      "eval: 0.871 0.31904104\n",
      "train 1300 0.3658003\n",
      "eval: 0.8722 0.31500906\n",
      "train 1400 0.3055213\n",
      "eval: 0.876 0.31335786\n",
      "train 1500 0.3676196\n",
      "eval: 0.8708 0.31350103\n",
      "train 1600 0.3299188\n",
      "eval: 0.871 0.306704\n",
      "train 1700 0.3098334\n",
      "eval: 0.8744 0.31187183\n",
      "train 1800 0.2705818\n",
      "eval: 0.8744 0.30541897\n",
      "train 1900 0.3090287\n",
      "eval: 0.8742 0.305995\n",
      "train 2000 0.43554428\n",
      "eval: 0.8766 0.3040884\n",
      "train 2100 0.29352897\n",
      "eval: 0.8722 0.30498534\n",
      "train 2200 0.2957105\n",
      "eval: 0.8796 0.2950864\n",
      "train 2300 0.33317405\n",
      "eval: 0.8806 0.2936384\n",
      "train 2400 0.3004605\n",
      "eval: 0.8764 0.30529824\n",
      "train 2500 0.2907606\n",
      "eval: 0.8818 0.29322663\n",
      "train 2600 0.3594063\n",
      "eval: 0.8746 0.29443344\n",
      "train 2700 0.28896138\n",
      "eval: 0.882 0.289065\n",
      "train 2800 0.2897342\n",
      "eval: 0.8802 0.29005957\n",
      "train 2900 0.33253652\n",
      "eval: 0.88 0.2901335\n",
      "train 3000 0.2757768\n",
      "eval: 0.8816 0.28900596\n",
      "train 3100 0.2682255\n",
      "eval: 0.8828 0.28542385\n",
      "train 3200 0.31240815\n",
      "eval: 0.8858 0.28509995\n",
      "train 3300 0.24150154\n",
      "eval: 0.8838 0.2878076\n",
      "train 3400 0.28311437\n",
      "eval: 0.8854 0.28463003\n",
      "train 3500 0.28714085\n",
      "eval: 0.8864 0.2797937\n",
      "train 3600 0.23701808\n",
      "eval: 0.8816 0.2848028\n",
      "train 3700 0.27334073\n",
      "eval: 0.8876 0.27883244\n",
      "train 3800 0.28381425\n",
      "eval: 0.8856 0.28476533\n",
      "train 3900 0.23787005\n",
      "eval: 0.8842 0.27901474\n",
      "train 4000 0.3024115\n",
      "eval: 0.8842 0.28672174\n",
      "train 4100 0.30938658\n",
      "eval: 0.8878 0.2746289\n",
      "train 4200 0.26131138\n",
      "eval: 0.8912 0.2758984\n",
      "train 4300 0.26035082\n",
      "eval: 0.8874 0.2758479\n",
      "train 4400 0.23830634\n",
      "eval: 0.8896 0.27389342\n",
      "train 4500 0.2462177\n",
      "eval: 0.886 0.2743394\n",
      "train 4600 0.24036539\n",
      "eval: 0.8872 0.27566877\n",
      "train 4700 0.23879053\n",
      "eval: 0.8864 0.2713339\n",
      "train 4800 0.26861042\n",
      "eval: 0.8908 0.27218613\n",
      "train 4900 0.26873887\n",
      "eval: 0.8862 0.2759639\n",
      "train 5000 0.25950432\n",
      "eval: 0.8902 0.2731096\n",
      "train 5100 0.25531745\n",
      "eval: 0.892 0.26987848\n",
      "train 5200 0.28191045\n",
      "eval: 0.8912 0.26888788\n",
      "train 5300 0.26180986\n",
      "eval: 0.8888 0.27057227\n",
      "train 5400 0.27287292\n",
      "eval: 0.89 0.26844838\n",
      "train 5500 0.24630609\n",
      "eval: 0.8902 0.27045804\n",
      "train 5600 0.24666527\n",
      "eval: 0.8918 0.26538116\n",
      "train 5700 0.26604518\n",
      "eval: 0.8912 0.26358432\n",
      "train 5800 0.2523096\n",
      "eval: 0.891 0.26724607\n",
      "train 5900 0.27513033\n",
      "eval: 0.8904 0.27096832\n",
      "train 6000 0.24936162\n",
      "eval: 0.8922 0.26579902\n",
      "train 6100 0.31206477\n",
      "eval: 0.895 0.25980386\n",
      "train 6200 0.27342343\n",
      "eval: 0.8924 0.26168004\n",
      "train 6300 0.28317523\n",
      "eval: 0.8904 0.26424733\n",
      "train 6400 0.2696917\n",
      "eval: 0.892 0.26108888\n",
      "train 6500 0.23666328\n",
      "eval: 0.893 0.2612782\n",
      "train 6600 0.27537394\n",
      "eval: 0.8966 0.2596212\n",
      "train 6700 0.24781679\n",
      "eval: 0.8954 0.26317832\n",
      "train 6800 0.22501898\n",
      "eval: 0.8942 0.2621526\n",
      "train 6900 0.23198666\n",
      "eval: 0.8982 0.25913697\n",
      "train 7000 0.34072572\n",
      "eval: 0.8948 0.25408536\n",
      "train 7100 0.25300163\n",
      "eval: 0.896 0.2546019\n",
      "train 7200 0.283117\n",
      "eval: 0.893 0.2554136\n",
      "train 7300 0.2557848\n",
      "eval: 0.8948 0.25005364\n",
      "train 7400 0.28075302\n",
      "eval: 0.8946 0.2587824\n",
      "train 7500 0.24260366\n",
      "eval: 0.8932 0.25623915\n",
      "train 7600 0.2825988\n",
      "eval: 0.8932 0.25967216\n",
      "train 7700 0.28338733\n",
      "eval: 0.8922 0.25666714\n",
      "train 7800 0.2727044\n",
      "eval: 0.895 0.2531386\n",
      "train 7900 0.25410265\n",
      "eval: 0.8962 0.25360614\n",
      "train 8000 0.2809413\n",
      "eval: 0.895 0.25141507\n",
      "train 8100 0.24469452\n",
      "eval: 0.8954 0.25209188\n",
      "train 8200 0.27627844\n",
      "eval: 0.8922 0.25825217\n",
      "train 8300 0.24239855\n",
      "eval: 0.8992 0.25070804\n",
      "train 8400 0.27684784\n",
      "eval: 0.8988 0.25284064\n",
      "train 8500 0.24058583\n",
      "eval: 0.901 0.25372064\n",
      "train 8600 0.21848723\n",
      "eval: 0.8984 0.25395614\n",
      "train 8700 0.27235234\n",
      "eval: 0.8966 0.2544051\n",
      "train 8800 0.2548698\n",
      "eval: 0.897 0.2507757\n",
      "train 8900 0.27809098\n",
      "eval: 0.8978 0.25112757\n",
      "train 9000 0.23160127\n",
      "eval: 0.8984 0.25637037\n",
      "train 9100 0.23104216\n",
      "eval: 0.8992 0.24918057\n",
      "train 9200 0.244304\n",
      "eval: 0.9004 0.24712875\n",
      "train 9300 0.23352173\n",
      "eval: 0.8966 0.24729775\n",
      "train 9400 0.23362485\n",
      "eval: 0.8968 0.24835844\n",
      "train 9500 0.25108773\n",
      "eval: 0.8966 0.2511998\n",
      "train 9600 0.26852062\n",
      "eval: 0.896 0.25083637\n",
      "train 9700 0.25659153\n",
      "eval: 0.8962 0.2521461\n",
      "train 9800 0.2218324\n",
      "eval: 0.8996 0.25455186\n",
      "train 9900 0.26687548\n",
      "eval: 0.8948 0.25116783\n"
     ]
    }
   ],
   "source": [
    "sub_observ=subsample(observation_list,subrate)\n",
    "for step in range(10000):\n",
    "#     this_batch = random.sample(list(zip(observation_list,a_list)),batch_size)\n",
    "    this_batch = random.sample(list(zip(sub_observ[:-5000],a_list[:-5000])),batch_size)\n",
    "    this_batch = list(zip(*this_batch))    \n",
    "    _,loss= training_step(network,this_batch[0],one_hot(this_batch[1],3))\n",
    "    if not step%100:\n",
    "        print('train',step, np.mean(loss))\n",
    "#         e_batch = [Xobservation_list,Xa_list]\n",
    "        e_batch = [sub_observ[-5000:],a_list[-5000:]]\n",
    "        e_actions= np.argmax(a_eval(network,e_batch[0]),axis=1)\n",
    "        e_loss = calc_loss(network,e_batch[0],one_hot(e_batch[1],3))\n",
    "        print('eval:',np.mean(e_actions==e_batch[1]),np.mean(e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nwk81xrelu_0p1_in128.nwk', 'wb') as f:\n",
    "    with sess.as_default():\n",
    "        pickle.dump(network.theta_values(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'localNetwork' object has no attribute 'sa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-eda6fe279e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'localNetwork' object has no attribute 'sa'"
     ]
    }
   ],
   "source": [
    "network.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot([0,1,2,3,2,1],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(100):\n",
    "    this_batch = random.sample(list(zip(observation_list,a_list)),batch_size)\n",
    "    this_batch = list(zip(*this_batch))    \n",
    "    these_actions= np.argmax(a_eval(network,this_batch[0]),axis=1)\n",
    "    loss = calc_loss(network,this_batch[0],one_hot(this_batch[1],3))\n",
    "    if not step%1:\n",
    "        print(step, np.mean(these_actions==this_batch[1]),np.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(observation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(e_batch[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sub_observ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
