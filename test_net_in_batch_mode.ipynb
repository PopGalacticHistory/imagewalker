{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import glob\n",
    "from misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localNetwork():\n",
    "    def __init__(self, n_features, n_actions, lr=None, trainable = False):\n",
    "        self.hp = HP()\n",
    "        #self.default_nl=tf.nn.relu\n",
    "        self.hp.lr = lr\n",
    "        self.next_layer_id = 0\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.theta = {}\n",
    "        self.estimator = self.vanilla_network()\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, n_actions])\n",
    "        if trainable:\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.q_target, logits=self.estimator)\n",
    "            # self.loss = tf.reduce_mean(tf.losses.absolute_difference(self.q_target, self.estimator))\n",
    "            # tf.losses.absolute_difference()\n",
    "            # self.train_op = tf.train.GradientDescentOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            # self.train_op = tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            self.train_op = tf.train.AdamOptimizer(self.hp.lr).minimize(self.loss)\n",
    "        self.sess = None\n",
    "\n",
    "    def get_layer_id(self):\n",
    "        this_layer_id = self.next_layer_id\n",
    "        self.next_layer_id +=1\n",
    "        return this_layer_id\n",
    "\n",
    "    def vanilla_network(self, layer_size = [None]+[200]*2+[ None]):\n",
    "        layer_size[0] = self.n_features\n",
    "        layer_size[-1] = self.n_actions\n",
    "        next_l = self.input_layer() #todo currently the  number of features in the input layer is defined elsewhere\n",
    "        self.observations = next_l\n",
    "        for ll, ll_size  in enumerate(layer_size[1:-1]):\n",
    "            next_l = self.dense_ff_layer(next_l, ll_size)\n",
    "            # next_l = tf.nn.dropout(next_l, 0.95)\n",
    "        ll_size=layer_size[-1]\n",
    "        next_l = self.dense_ff_layer(next_l, ll_size, nl= lambda x: x,g=1e-10)\n",
    "        return next_l\n",
    "\n",
    "\n",
    "    def dense_ff_layer(self, previous_layer, output_size, nl=tf.nn.relu, theta = None,g=1.0):\n",
    "        if theta is None:\n",
    "            this_theta = {}\n",
    "            # print(np.float(np.shape(previous_layer)[-1])**0.5)\n",
    "            this_theta['w'] = tf.Variable(\n",
    "                tf.random_normal(shape=[np.shape(previous_layer)[-1].value, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=g*2.0 / np.sqrt(np.shape(previous_layer)[-1].value)))\n",
    "            this_theta['b'] = tf.Variable(\n",
    "                tf.random_normal(shape=[1, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=0.01))\n",
    "        else:\n",
    "            error('explicit theta is still unsupported')\n",
    "        self.theta[self.get_layer_id()] = this_theta\n",
    "        #print(self.get_layer_id())\n",
    "        ff_layer = nl(tf.matmul(previous_layer, this_theta['w']) + this_theta['b'])\n",
    "        return ff_layer\n",
    "\n",
    "    def input_layer(self):\n",
    "        return tf.placeholder(tf.float32, [None, self.n_features])\n",
    "\n",
    "    # def train_step_op(self):\n",
    "    #     return tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "\n",
    "    def assign_param_prep(self,source_nwk): #todo support more elaborated structures than double dictionary\n",
    "        self.assign_param_op = []\n",
    "        for ll in source_nwk.theta.keys():\n",
    "            for this_param in source_nwk.theta[ll]:\n",
    "                self.assign_param_op.append(tf.assign(self.theta[ll][this_param],\n",
    "                                                 source_nwk.theta[ll][this_param]))\n",
    "\n",
    "    def theta_values(self): #todo support more elaborated structures than double dictionary\n",
    "        t = {}\n",
    "        for ll in self.theta.keys():\n",
    "            t[ll] = {}\n",
    "            for this_param in self.theta[ll]:\n",
    "                t[ll][this_param] = self.theta[ll][this_param].eval(self.sess)\n",
    "        return t\n",
    "\n",
    "    def theta_update(self,t): #todo support more elaborated structures than double dictionary\n",
    "        for ll in t.keys():\n",
    "            for this_param in t[ll]:\n",
    "                self.theta[ll][this_param].assign(t[ll][this_param]).op.run(session=self.sess)\n",
    "\n",
    "    def update(self, sess):\n",
    "        sess.run(self.assign_param_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(nwk, observations, q_target):\n",
    "        return sess.run([nwk.train_op,nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "\n",
    "    def calc_loss(nwk, observations, q_target):\n",
    "        return sess.run([nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "    \n",
    "    def a_eval(nwk,observations):\n",
    "        return sess.run(nwk.estimator,\n",
    "                  feed_dict={nwk.observations: observations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(a,depth):\n",
    "    o=np.zeros([len(a),depth])\n",
    "    o[list(range(len(a))),a]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = localNetwork( 256*4, 3, lr=0.0025, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_observation_action_from_path(path = None, filenames = None, max_file=1e7):\n",
    "    if filenames is None:\n",
    "        filenames = sorted(glob.glob(path))\n",
    "    observation_list=[]\n",
    "    a_list=[]\n",
    "    for cnt, this_file in enumerate(filenames):\n",
    "        if cnt<max_file:\n",
    "            with open(this_file,'rb') as f:\n",
    "                observation_list_s, a_list_s = pickle.load(f)\n",
    "                observation_list += observation_list_s\n",
    "                a_list += a_list_s\n",
    "        else:\n",
    "            break\n",
    "    return observation_list,a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xobservation_list,Xa_list = read_observation_action_from_path('XXobservation_action_dump_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nwk200x200xrelu_0p1.nwk', 'rb') as f:\n",
    "    with sess.as_default():\n",
    "        theta_list = pickle.load(f)\n",
    "        network.theta_update(theta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.794921875 3.8290036\n",
      "1 0.7578125 4.7180567\n",
      "2 0.765625 6.643584\n",
      "3 0.771484375 3.5995011\n",
      "4 0.771484375 5.1158695\n",
      "5 0.740234375 5.138471\n",
      "6 0.76953125 7.1452675\n",
      "7 0.75 4.7170377\n",
      "8 0.791015625 3.1270542\n",
      "9 0.765625 5.439272\n",
      "10 0.775390625 5.6071243\n",
      "11 0.783203125 4.9340715\n",
      "12 0.75 7.4632463\n",
      "13 0.767578125 3.67767\n",
      "14 0.7578125 7.266601\n",
      "15 0.76171875 4.2382774\n",
      "16 0.748046875 5.8354063\n",
      "17 0.74609375 7.99998\n",
      "18 0.7578125 5.5590196\n",
      "19 0.732421875 6.5605264\n",
      "20 0.765625 7.5470247\n",
      "21 0.767578125 3.3008876\n",
      "22 0.76953125 5.626027\n",
      "23 0.763671875 7.414331\n",
      "24 0.765625 4.010283\n",
      "25 0.75390625 4.648808\n",
      "26 0.763671875 4.371417\n",
      "27 0.7734375 5.1772175\n",
      "28 0.75 7.2572346\n",
      "29 0.779296875 6.2004766\n",
      "30 0.767578125 4.05719\n",
      "31 0.775390625 4.143829\n",
      "32 0.73828125 8.332233\n",
      "33 0.77734375 5.7977467\n",
      "34 0.787109375 4.854104\n",
      "35 0.755859375 4.6827345\n",
      "36 0.783203125 4.6360598\n",
      "37 0.78125 4.155876\n",
      "38 0.7421875 6.3077455\n",
      "39 0.7265625 3.9159138\n",
      "40 0.7734375 3.9879005\n",
      "41 0.759765625 6.380677\n",
      "42 0.744140625 6.4409037\n",
      "43 0.75390625 5.2806935\n",
      "44 0.78125 3.5602689\n",
      "45 0.76171875 5.227848\n",
      "46 0.7421875 4.6460953\n",
      "47 0.759765625 4.5502377\n",
      "48 0.76953125 4.647113\n",
      "49 0.765625 6.013648\n",
      "50 0.767578125 6.8799524\n",
      "51 0.73046875 5.701105\n",
      "52 0.75 7.4786224\n",
      "53 0.76953125 5.22094\n",
      "54 0.740234375 4.1032543\n",
      "55 0.7578125 6.085476\n",
      "56 0.74609375 4.9313426\n",
      "57 0.763671875 5.3557615\n",
      "58 0.75390625 5.9298973\n",
      "59 0.775390625 3.3949475\n",
      "60 0.76953125 4.06918\n",
      "61 0.7734375 4.483294\n",
      "62 0.783203125 5.635519\n",
      "63 0.78125 4.0296564\n",
      "64 0.7578125 7.3816547\n",
      "65 0.75390625 6.8963842\n",
      "66 0.76953125 7.5332985\n",
      "67 0.765625 4.5488887\n",
      "68 0.748046875 7.1856604\n",
      "69 0.76953125 4.700702\n",
      "70 0.76171875 4.615125\n",
      "71 0.7578125 7.3663034\n",
      "72 0.748046875 6.366763\n",
      "73 0.7890625 5.700924\n",
      "74 0.7421875 4.4854074\n",
      "75 0.73828125 5.9547005\n",
      "76 0.7890625 5.3216844\n",
      "77 0.794921875 5.555495\n",
      "78 0.740234375 5.7600074\n",
      "79 0.791015625 5.143211\n",
      "80 0.76171875 5.678131\n",
      "81 0.775390625 5.1383452\n",
      "82 0.787109375 4.179659\n",
      "83 0.767578125 8.127842\n",
      "84 0.75390625 6.476809\n",
      "85 0.7578125 3.6350636\n",
      "86 0.75 6.146048\n",
      "87 0.74609375 4.9298778\n",
      "88 0.76953125 6.101306\n",
      "89 0.77734375 4.71548\n",
      "90 0.77734375 3.5889564\n",
      "91 0.732421875 7.0628567\n",
      "92 0.7734375 5.6050787\n",
      "93 0.759765625 4.7022305\n",
      "94 0.759765625 4.624963\n",
      "95 0.779296875 5.037533\n",
      "96 0.78515625 4.9298587\n",
      "97 0.7578125 6.1500006\n",
      "98 0.7734375 3.9870803\n",
      "99 0.791015625 3.241137\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    this_batch = random.sample(list(zip(Xobservation_list,Xa_list)),batch_size)\n",
    "    this_batch = list(zip(*this_batch))    \n",
    "    these_actions= np.argmax(a_eval(network,this_batch[0]),axis=1)\n",
    "    loss = calc_loss(network,this_batch[0],one_hot(this_batch[1],3))\n",
    "    if not step%1:\n",
    "        print(step, np.mean(these_actions==this_batch[1]),np.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7644071781208369 5.379124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "e_batch = [Xobservation_list,Xa_list]\n",
    "e_actions= np.argmax(a_eval(network,e_batch[0]),axis=1)\n",
    "e_loss = calc_loss(network,e_batch[0],one_hot(e_batch[1],3))\n",
    "print(np.mean(e_actions==e_batch[1]),np.mean(e_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
