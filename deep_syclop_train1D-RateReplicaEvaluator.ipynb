{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DVS128 closed loop oscillator routine by Alexander Rivkind, Eldad Assa, Michael Kreiserman and Ehud Ahissar\n",
    "based on: \n",
    "*DVS example by: Yuhuang Hu (duguyue100@gmail.com)\n",
    "*\"Syclop\" paradigm: Ahissar and Assa 2016\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import SYCLOP_env as syc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pickle\n",
    "from RL_brain_b import DeepQNetwork\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import copy\n",
    "from misc import *\n",
    "import maestro\n",
    "import sys\n",
    "import os\n",
    "from pyaer.dvs128 import DVS128\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localNetwork():\n",
    "    def __init__(self, n_features, n_actions, lr=None, trainable = False):\n",
    "        self.hp = HP()\n",
    "        #self.default_nl=tf.nn.relu\n",
    "        self.hp.lr = lr\n",
    "        self.next_layer_id = 0\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.theta = {}\n",
    "        self.estimator = self.vanilla_network()\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, n_actions])\n",
    "        if trainable:\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.q_target, logits=self.estimator)\n",
    "            # self.loss = tf.reduce_mean(tf.losses.absolute_difference(self.q_target, self.estimator))\n",
    "            # tf.losses.absolute_difference()\n",
    "            # self.train_op = tf.train.GradientDescentOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            # self.train_op = tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "            self.train_op = tf.train.AdamOptimizer(self.hp.lr).minimize(self.loss)\n",
    "        self.sess = None\n",
    "\n",
    "    def get_layer_id(self):\n",
    "        this_layer_id = self.next_layer_id\n",
    "        self.next_layer_id +=1\n",
    "        return this_layer_id\n",
    "\n",
    "    def vanilla_network(self, layer_size = [None]+[200]*2+[ None]):\n",
    "        layer_size[0] = self.n_features\n",
    "        layer_size[-1] = self.n_actions\n",
    "        next_l = self.input_layer() #todo currently the  number of features in the input layer is defined elsewhere\n",
    "        self.observations = next_l\n",
    "        for ll, ll_size  in enumerate(layer_size[1:-1]):\n",
    "            next_l = self.dense_ff_layer(next_l, ll_size)\n",
    "            next_l = tf.nn.dropout(next_l, 0.98)\n",
    "        ll_size=layer_size[-1]\n",
    "        next_l = self.dense_ff_layer(next_l, ll_size, nl= lambda x: x,g=1e-10)\n",
    "        return next_l\n",
    "\n",
    "\n",
    "    def dense_ff_layer(self, previous_layer, output_size, nl=tf.nn.relu, theta = None,g=1.0):\n",
    "        if theta is None:\n",
    "            this_theta = {}\n",
    "            # print(np.float(np.shape(previous_layer)[-1])**0.5)\n",
    "            this_theta['w'] = tf.Variable(\n",
    "                tf.random_normal(shape=[np.shape(previous_layer)[-1].value, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=g*2.0 / np.sqrt(np.shape(previous_layer)[-1].value)))\n",
    "            this_theta['b'] = tf.Variable(\n",
    "                tf.random_normal(shape=[1, output_size],\n",
    "                                 mean=0.0,\n",
    "                                 stddev=0.01))\n",
    "        else:\n",
    "            error('explicit theta is still unsupported')\n",
    "        self.theta[self.get_layer_id()] = this_theta\n",
    "        #print(self.get_layer_id())\n",
    "        ff_layer = nl(tf.matmul(previous_layer, this_theta['w']) + this_theta['b'])\n",
    "        return ff_layer\n",
    "\n",
    "    def input_layer(self):\n",
    "        return tf.placeholder(tf.float32, [None, self.n_features])\n",
    "\n",
    "    # def train_step_op(self):\n",
    "    #     return tf.train.RMSPropOptimizer(self.hp.lr).minimize(self.loss)\n",
    "\n",
    "    def assign_param_prep(self,source_nwk): #todo support more elaborated structures than double dictionary\n",
    "        self.assign_param_op = []\n",
    "        for ll in source_nwk.theta.keys():\n",
    "            for this_param in source_nwk.theta[ll]:\n",
    "                self.assign_param_op.append(tf.assign(self.theta[ll][this_param],\n",
    "                                                 source_nwk.theta[ll][this_param]))\n",
    "\n",
    "    def theta_values(self): #todo support more elaborated structures than double dictionary\n",
    "        t = {}\n",
    "        for ll in self.theta.keys():\n",
    "            t[ll] = {}\n",
    "            for this_param in self.theta[ll]:\n",
    "                t[ll][this_param] = self.theta[ll][this_param].eval(self.sess)\n",
    "        return t\n",
    "\n",
    "    def theta_update(self,t): #todo support more elaborated structures than double dictionary\n",
    "        for ll in t.keys():\n",
    "            for this_param in t[ll]:\n",
    "                self.theta[ll][this_param].assign(t[ll][this_param]).op.run(session=self.sess)\n",
    "\n",
    "    def update(self, sess):\n",
    "        sess.run(self.assign_param_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def training_step(nwk, observations, q_target):\n",
    "        return sess.run([nwk.train_op,nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "\n",
    "    def calc_loss(nwk, observations, q_target):\n",
    "        return sess.run([nwk.loss],\n",
    "                  feed_dict={nwk.observations: observations,\n",
    "                             nwk.q_target: q_target})\n",
    "    \n",
    "    def a_eval(nwk,observations):\n",
    "        return sess.run(nwk.estimator,\n",
    "                  feed_dict={nwk.observations: observations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Free_run_agent():\n",
    "    def __init__(self,max_q = None):\n",
    "        self.hp = HP()\n",
    "#         self.hp.action_space = ['v_right','v_left','v_up','v_down','null'] + \\\n",
    "#                                [['v_right','v_up'],['v_right','v_down'],['v_left','v_up'],['v_left','v_down']]#\n",
    "        self.hp.action_space = ['v_right','v_left','null']\n",
    "        self.hp.max_speed  = 10\n",
    "        self.q_centre = np.array([0.,0.], dtype='f') \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_ana = np.array([0.,0.], dtype='f') \n",
    "        self.qdot = np.array([0.0,0.0])\n",
    "        self.qdotdot = np.array([0.0,0.0])\n",
    "        self.q = np.int32(np.floor(self.q_ana))\n",
    "\n",
    "    def sync_with_servo(self,servo,orientations = [0,1]):\n",
    "        for orientation in orientations:\n",
    "            if not servo.isMoving(orientation):\n",
    "                self.qdot[orientation]=0.0\n",
    "                \n",
    "            \n",
    "    def act(self,a):\n",
    "        if a is None:\n",
    "            action = 'null'\n",
    "        else:\n",
    "            action = self.hp.action_space[a]\n",
    "        #delta_a = 0.001\n",
    "        if type(action) == list:\n",
    "            for subaction in action:\n",
    "                self.parse_action(subaction)\n",
    "        else:\n",
    "            self.parse_action(action)\n",
    "\n",
    "        #print('debug', self.max_q, self.q_centre)\n",
    "        self.qdot += self.qdotdot\n",
    "        #self.qdot -= self.hp.returning_force*(self.q_ana-self.q_centre)\n",
    "        self.q_ana +=self.qdot\n",
    "        self.q = np.int32(np.floor(self.q_ana))\n",
    "\n",
    "    def parse_action(self,action):\n",
    "        if type(action)==int:\n",
    "            self.qdot[0] = action\n",
    "            self.qdotdot = np.array([0., 0.])\n",
    "        elif action == 'v_up':   # up\n",
    "            self.qdot[1] = self.qdot[1] + (1 if self.qdot[1] < self.hp.max_speed else 0)\n",
    "            self.qdotdot = np.array([0., 0.])\n",
    "        elif action == 'v_down':   # down\n",
    "            self.qdot[1] = self.qdot[1] - (1 if self.qdot[1] > -self.hp.max_speed else 0)  \n",
    "            self.qdotdot = np.array([0., 0.])\n",
    "        elif action == 'v_left':   # left\n",
    "            self.qdot[0] = self.qdot[0] - (1 if self.qdot[0] > -self.hp.max_speed else 0)  \n",
    "            self.qdotdot = np.array([0., 0.])\n",
    "        elif action == 'v_right':   # right\n",
    "            self.qdot[0] = self.qdot[0] + (1 if self.qdot[0] < self.hp.max_speed else 0) \n",
    "            self.qdotdot = np.array([0.,0.])\n",
    "        elif action == 'null':   # null\n",
    "            pass\n",
    "        else:\n",
    "            error('unknown action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HP()\n",
    "hp.mem_depth=1\n",
    "hp.logmode = False\n",
    "hp.steps_between_learnings = 100\n",
    "hp.save_path = 'saved_runs'\n",
    "hp.this_run_name = 'rateSpikeGames' + '_noname_' + str(int(time.time()))\n",
    "hp.description = \"work on copying cyclop into rates\"\n",
    "hp.mem_depth = 1\n",
    "hp.max_episode =  10000\n",
    "hp.steps_per_episode = 1000\n",
    "hp.steps_between_learnings = 100\n",
    "hp.fading_mem = 0.5\n",
    "recorder_file = 'records.pkl'\n",
    "hp_file = 'hp.pkl'\n",
    "hp.contrast_range = [1.0,1.1]\n",
    "hp.logmode = False\n",
    "hp.dt_nom=0.025\n",
    "#     recorder = Recorder(n=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(hp.save_path):\n",
    "    os.makedirs(hp.save_path)\n",
    "\n",
    "hp.this_run_path = hp.save_path+'/'+hp.this_run_name+'/'\n",
    "if not os.path.exists(hp.this_run_path):\n",
    "    os.makedirs(hp.this_run_path)\n",
    "else:\n",
    "    error('run name already exists!')\n",
    "\n",
    "\n",
    "with open(hp.this_run_path+hp_file, 'wb') as f:\n",
    "    pickle.dump(hp, f)\n",
    "\n",
    "sensor = syc.Sensor()\n",
    "agent = Free_run_agent()\n",
    "\n",
    "reward = syc.Rewards()\n",
    "observation_size = 256*4\n",
    "# RL = DeepQNetwork(len(agent.hp.action_space), observation_size*hp.mem_depth,#sensor.frame_size+2,\n",
    "#                   reward_decay=0.99,\n",
    "#                   e_greedy=0.999,\n",
    "#                   e_greedy0=0.999,\n",
    "#                   replace_target_iter=10,\n",
    "#                   memory_size=100000,\n",
    "#                   e_greedy_increment=0.0001,\n",
    "#                   learning_rate=0.0025,\n",
    "#                   double_q=False,\n",
    "#                   dqn_mode=True,\n",
    "#                   state_table=np.zeros([1,observation_size*hp.mem_depth])\n",
    "#                   )\n",
    "# #     RL.dqn.load_nwk_param('saved_runs/run_syclop_lirondb.py_noname_1557658952/best_liron.nwk')\n",
    "# #     RL.dqn.load_nwk_param('zhoka.nwk')\n",
    "# #     RL.dqn.load_nwk_param('saved_runs/old_flat_saves/liron_random_ic05.nwk')#   \n",
    "# RL.dqn.load_nwk_param('Telluride_candidate1.nwk')\n",
    "\n",
    "network = localNetwork( 256*4, 3, lr=0.0025, trainable = True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with open('nwk200x200xrelu_0p1_d0p5.nwk', 'rb') as f:\n",
    "    with sess.as_default():\n",
    "        theta_list = pickle.load(f)\n",
    "        network.theta_update(theta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "servo = maestro.Controller(ttyStr='/dev/ttyACM1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gohome():    \n",
    "    servo.setSpeed(0,0)\n",
    "    servo.setSpeed(1,0)\n",
    "    servo.setTarget(1,7500)     #set speed of servo 1\n",
    "    servo.setTarget(0,5000)  #set servo to move to center position\n",
    "    servo.setSpeed(0,1)\n",
    "    servo.setSpeed(1,1)\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gohome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "servo.moveByVelocity(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_observer(sensor,agent):\n",
    "    if hp.logmode:\n",
    "        normfactor=1.0\n",
    "    else:\n",
    "        normfactor = 1.0/1.0\n",
    "    return normfactor*np.concatenate([relu_up_and_down(sensor.central_dvs_view),\n",
    "            relu_up_and_down(cv2.resize(1.0*sensor.dvs_view, dsize=(16, 16), interpolation=cv2.INTER_AREA))])\n",
    "\n",
    "observation = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n",
    "hp.fading_mem = 0.5\n",
    "recorder = Recorder(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(hp.this_run_path+hp_file, 'wb') as f:\n",
    "        pickle.dump(hp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going home!\n",
      "step 0\n",
      "0 1000  running reward    0.17799220157684117\n",
      "going home!\n",
      "step 1000\n",
      "1 2000  running reward    0.2233205787489007\n",
      "going home!\n",
      "step 2000\n",
      "2 3000  running reward    0.20625911077959996\n",
      "going home!\n",
      "step 3000\n",
      "3 4000  running reward    0.2410918739583523\n",
      "going home!\n",
      "step 4000\n",
      "4 5000  running reward    0.24667953746668553\n",
      "going home!\n",
      "step 5000\n",
      "5 6000  running reward    0.26446082220179706\n",
      "going home!\n",
      "step 6000\n",
      "going home!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c6f895856f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             (pol_events, num_pol_event,\n\u001b[1;32m     39\u001b[0m              special_events, num_special_event) = \\\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"events_hist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt_this\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_prev\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdt_nom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mt_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "training = True\n",
    "device = DVS128()\n",
    "\n",
    "\n",
    "device.start_data_stream()\n",
    "# load new config\n",
    "# device.set_bias_from_json(\"./scripts/configs/dvs128_config.json\")\n",
    "# print (device.get_bias())\n",
    "\n",
    "clip_value = 3\n",
    "histrange = [(0, v) for v in (128, 128)]\n",
    "t_prev = time.time()\n",
    "dt_nom = hp.dt_nom\n",
    "episode = 0\n",
    "# dt_list = []\n",
    "cnt = 0\n",
    "empty_cnt = 0\n",
    "best_thus_far = 0.0\n",
    "step = 0 \n",
    "observation = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n",
    "observation_ = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n",
    "running_ave_reward = 0\n",
    "agent.reset()\n",
    "t0 = time.time()\n",
    "for episode in range(hp.max_episode):\n",
    "    gohome()\n",
    "    print('going home!')\n",
    "\n",
    "    observation = 0*local_observer(sensor, agent)\n",
    "    observation_ = 0*local_observer(sensor, agent)\n",
    "    agent.reset()\n",
    "    step_prime=0\n",
    "    observation_list=[]\n",
    "    a_list=[]\n",
    "    while step_prime < hp.steps_per_episode:\n",
    "        t_this = time.time()\n",
    "        try:\n",
    "            (pol_events, num_pol_event,\n",
    "             special_events, num_special_event) = \\\n",
    "                device.get_event(\"events_hist\")\n",
    "            if t_this - t_prev > dt_nom:\n",
    "                    t_prev=t_this\n",
    "                    if num_pol_event != 0:\n",
    "                        img = pol_events[..., 1]-pol_events[..., 0]\n",
    "                        cv2.imshow(\"image\", np.flip(np.flip( img/float(clip_value*2),axis=0),axis=1))\n",
    "#                         action = RL.choose_action(observation.reshape([-1]))\n",
    "                        action= np.argmax(a_eval(network,observation.reshape([1,-1])),axis=1)[0]\n",
    "                        observation_list.append(observation)\n",
    "                        a_list.append(action)\n",
    "                        reward.update_rewards(sensor = sensor, agent = agent)\n",
    "                        running_ave_reward = 0.999*running_ave_reward+0.001*reward.reward\n",
    "    #                     reward.update_rewards(sensor = sensor, agent = agent)\n",
    "                        recorder.record([agent.q_ana[0],agent.q_ana[1], agent.qdot[0],agent.qdot[1],reward.reward,t_this,2])\n",
    "                        agent.sync_with_servo(servo)\n",
    "                        agent.act(action)\n",
    "                        for orientation in [0,1]:\n",
    "                            servo.moveByVelocity(orientation,(int(np.round(agent.qdot[orientation]))))\n",
    "                        sensor.dvs_view = cv2.resize(1.0*img, dsize=(64, 64))\n",
    "                        sensor.central_dvs_view = cv2.resize(1.0*img[64-8:64:8,64-8:64+8], dsize=(16, 16))\n",
    "                        if step_prime == 0:\n",
    "                            sensor.dvs_view *= 0.\n",
    "                            sensor.central_dvs_view *= 0.\n",
    "    #                     print(np.min(sensor.dvs_view ),np.max(sensor.dvs_view ) )\n",
    "                        observation_ *= hp.fading_mem\n",
    "                        observation_ += local_observer(sensor, agent)  # todo: generalize\n",
    "                        if step%1000 ==0:\n",
    "                            print('step',step)\n",
    "#                         RL.store_transition(observation.reshape([-1]), action, reward.reward, observation_.reshape([-1]))\n",
    "                        observation = copy.copy(observation_)\n",
    "                        step += 1\n",
    "                        step_prime +=1\n",
    "                        if  (step > 100) and (step % hp.steps_between_learnings == 0):\n",
    "                            t_pre=time.time()\n",
    "#                             RL.learn()\n",
    "                            t_post=time.time()\n",
    "                            if running_ave_reward > best_thus_far:\n",
    "                                best_thus_far = running_ave_reward\n",
    "#                                 RL.dqn.save_nwk_param(hp.this_run_path+'best_live.nwk')\n",
    "#                                 print('saved best network, mean reward: ', best_thus_far)\n",
    "#                             print('training took', t_post-t_pre)\n",
    "                        if step%10000 ==0:\n",
    "                            recorder.plot()\n",
    "#                             RL.dqn.save_nwk_param(hp.this_run_path+'tempX_1.nwk')\n",
    "                    # debug_policy_plot()\n",
    "                        if step % 10000 == 0:\n",
    "                                recorder.save(hp.this_run_path+recorder_file)\n",
    "                        if step%1000 ==0:\n",
    "                            print(episode,step,' running reward   ',running_ave_reward)\n",
    "                            \n",
    "        except KeyboardInterrupt:\n",
    "                device.shutdown()\n",
    "                break\n",
    "#     with open('Testing_observation_action_dump_'+str(episode)+'.pkl','wb') as f:\n",
    "#         pickle.dump([observation_list, a_list],f)\n",
    " \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.dqn.save_nwk_param('deleteme.nwk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.dqn.load_nwk_param('saved_runs//usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py_noname_1558597678/best_live.nwk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.this_run_path+'best_live.nwk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL.epsilon=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.log10(np.diff(recorder.records[-2][:2000])-np.log10(hp.dt_nom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot((np.diff(recorder.records[-2][:2000])/(hp.dt_nom)))\n",
    "plt.ylim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist((np.diff(recorder.records[-2][:2000])/(hp.dt_nom)),log=True,bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu=Recorder(n=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu.load('saved_runs//usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py_noname_1558603217/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
