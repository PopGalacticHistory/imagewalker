{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from misc import *\n",
    "import SYCLOP_env as syc\n",
    "from RL_brain_b import DeepQNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "mnist = MNIST('/home/bnapp/datasets/mnist/')\n",
    "images, labels = mnist.load_training()\n",
    "# some_mnistSM =[ cv2.resize(1.+np.reshape(uu,[28,28]), dsize=(256, 256)) for uu in images[:2]]#[:4096]]   \n",
    "some_samples_for_setup= prep_mnist_padded_images(2)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dir = 'saved_runs/run_syclop_generic1.py_noname_1576060868_0/' #padded mnist beta 0.1 speed penalty 5\n",
    "# result_type = 'nwk2.nwk'\n",
    "\n",
    "# run_dir = 'saved_runs/run_syclop_generic1.py_noname_1576147784_0/' #padded mnist beta 0.1 speed penalty 20\n",
    "\n",
    "\n",
    "run_dir = 'saved_runs/run_syclop_generic1.py_noname_1576403573_0/'  #padded mnist beta 0.1 speed penalty 0\n",
    "result_type = 'tempX_1.nwk'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug n_features: 1024\n",
      "debug: (?, 1024)\n",
      "debug: (?, 400)\n",
      "debug: (?, 200)\n",
      "debug: (?, 200)\n",
      "debug: (?, 200)\n",
      "debug n_features: 1024\n",
      "debug: (?, 1024)\n",
      "debug: (?, 400)\n",
      "debug: (?, 200)\n",
      "debug: (?, 200)\n",
      "debug: (?, 200)\n"
     ]
    }
   ],
   "source": [
    "hp = HP()\n",
    "hp.mem_depth=1\n",
    "hp.logmode=False\n",
    "batch_size=256\n",
    "action_space_size=9\n",
    "# images = some_mnistSM\n",
    "number_of_images = len(images)\n",
    "reward = syc.Rewards()\n",
    "observation_size = 256*4\n",
    "RL = DeepQNetwork(action_space_size, observation_size*hp.mem_depth,#sensor.frame_size+2,\n",
    "                  reward_decay=0.99,\n",
    "                  e_greedy=1-1e-9,\n",
    "                  e_greedy0=1-1e-9,\n",
    "                  replace_target_iter=10,\n",
    "                  memory_size=100000,\n",
    "                  e_greedy_increment=0.0001,\n",
    "                  learning_rate=0.0025,\n",
    "                  double_q=False,\n",
    "                  dqn_mode=True,\n",
    "                  state_table=np.zeros([1,observation_size*hp.mem_depth]),\n",
    "                  soft_q_type='boltzmann',\n",
    "                  beta=0.1\n",
    "                  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def local_observer(sensor,agent):\n",
    "    if hp.logmode:\n",
    "        normfactor=1.0\n",
    "    else:\n",
    "        normfactor = 1.0/256.0\n",
    "    return normfactor*np.concatenate([relu_up_and_down(sensor.central_dvs_view),\n",
    "            relu_up_and_down(cv2.resize(1.0*sensor.dvs_view, dsize=(16, 16), interpolation=cv2.INTER_AREA))])\n",
    "\n",
    "observation = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_bb = [None]*batch_size\n",
    "sensor_bb =[None]*batch_size\n",
    "agent_bb = [None]*batch_size\n",
    "action_bb = [None]*batch_size\n",
    "action_list_bb =  [None]*batch_size\n",
    "q_list_bb =  [None]*batch_size\n",
    "observation_bb = [None]*batch_size\n",
    "\n",
    "with open(run_dir+'/hp.pkl','rb') as f:\n",
    "                this_hp = pickle.load(f)\n",
    "\n",
    "for bb in range(batch_size):\n",
    "    scene_bb[bb] = syc.Scene(frame_list=some_samples_for_setup[0:1])\n",
    "    sensor_bb[bb] = syc.Sensor()\n",
    "    agent_bb[bb] = syc.Agent(max_q = [scene_bb[bb].maxx-sensor_bb[bb].hp.winx,scene_bb[bb].maxy-sensor_bb[bb].hp.winy])\n",
    "    agent_bb[bb].hp.action_space = this_hp.agent.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num: 0 wall time consumed: 39.24245738983154\n",
      "batch num: 1 wall time consumed: 38.755786180496216\n",
      "batch num: 2 wall time consumed: 38.64829134941101\n",
      "batch num: 3 wall time consumed: 38.89785671234131\n",
      "batch num: 4 wall time consumed: 38.728580713272095\n",
      "batch num: 5 wall time consumed: 38.78235936164856\n",
      "batch num: 6 wall time consumed: 38.581660747528076\n",
      "batch num: 7 wall time consumed: 39.75066304206848\n",
      "batch num: 8 wall time consumed: 39.06776213645935\n",
      "batch num: 9 wall time consumed: 38.79161477088928\n",
      "batch num: 10 wall time consumed: 38.849507093429565\n",
      "batch num: 11 wall time consumed: 39.05150532722473\n",
      "batch num: 12 wall time consumed: 39.012853384017944\n",
      "batch num: 13 wall time consumed: 38.74488806724548\n",
      "batch num: 14 wall time consumed: 38.79856014251709\n",
      "batch num: 15 wall time consumed: 38.77446150779724\n",
      "batch num: 16 wall time consumed: 38.767223834991455\n",
      "batch num: 17 wall time consumed: 39.92315196990967\n",
      "batch num: 18 wall time consumed: 38.765279054641724\n",
      "batch num: 19 wall time consumed: 38.805659532547\n",
      "batch num: 20 wall time consumed: 38.73130774497986\n",
      "batch num: 21 wall time consumed: 39.27479529380798\n",
      "batch num: 22 wall time consumed: 38.756571769714355\n",
      "batch num: 23 wall time consumed: 38.45664358139038\n",
      "batch num: 24 wall time consumed: 38.717315435409546\n",
      "batch num: 25 wall time consumed: 39.61017894744873\n",
      "batch num: 26 wall time consumed: 39.40128445625305\n",
      "batch num: 27 wall time consumed: 39.47049260139465\n",
      "batch num: 28 wall time consumed: 38.7383348941803\n",
      "batch num: 29 wall time consumed: 39.735931634902954\n",
      "batch num: 30 wall time consumed: 40.09104347229004\n",
      "batch num: 31 wall time consumed: 41.66463255882263\n",
      "batch num: 32 wall time consumed: 40.277501344680786\n",
      "batch num: 33 wall time consumed: 40.56296110153198\n",
      "batch num: 34 wall time consumed: 39.97669959068298\n",
      "batch num: 35 wall time consumed: 39.914886236190796\n",
      "batch num: 36 wall time consumed: 40.13045620918274\n",
      "batch num: 37 wall time consumed: 40.38657307624817\n",
      "batch num: 38 wall time consumed: 39.783586263656616\n",
      "batch num: 39 wall time consumed: 40.028661489486694\n",
      "batch num: 40 wall time consumed: 40.360772371292114\n",
      "batch num: 41 wall time consumed: 41.30896306037903\n",
      "batch num: 42 wall time consumed: 39.71547174453735\n",
      "batch num: 43 wall time consumed: 40.25914645195007\n",
      "batch num: 44 wall time consumed: 40.036412477493286\n",
      "batch num: 45 wall time consumed: 40.37951850891113\n",
      "batch num: 46 wall time consumed: 39.95870351791382\n",
      "batch num: 47 wall time consumed: 39.82537055015564\n",
      "batch num: 48 wall time consumed: 40.28987503051758\n",
      "batch num: 49 wall time consumed: 40.06290245056152\n",
      "batch num: 50 wall time consumed: 40.1447548866272\n",
      "batch num: 51 wall time consumed: 40.22420334815979\n",
      "batch num: 52 wall time consumed: 40.22356343269348\n",
      "batch num: 53 wall time consumed: 40.17773962020874\n",
      "batch num: 54 wall time consumed: 40.32162666320801\n",
      "batch num: 55 wall time consumed: 40.12950134277344\n",
      "batch num: 56 wall time consumed: 40.187488079071045\n",
      "batch num: 57 wall time consumed: 39.988410234451294\n",
      "batch num: 58 wall time consumed: 39.85330414772034\n",
      "batch num: 59 wall time consumed: 39.88715052604675\n",
      "batch num: 60 wall time consumed: 40.449333906173706\n",
      "batch num: 61 wall time consumed: 39.92989110946655\n",
      "batch num: 62 wall time consumed: 39.81858730316162\n",
      "batch num: 63 wall time consumed: 39.919777631759644\n",
      "batch num: 64 wall time consumed: 40.10007667541504\n",
      "batch num: 65 wall time consumed: 40.12239623069763\n",
      "batch num: 66 wall time consumed: 40.0331175327301\n",
      "batch num: 67 wall time consumed: 39.88394594192505\n",
      "batch num: 68 wall time consumed: 40.360435247421265\n",
      "batch num: 69 wall time consumed: 39.98176693916321\n",
      "batch num: 70 wall time consumed: 39.97550654411316\n",
      "batch num: 71 wall time consumed: 40.17575478553772\n",
      "batch num: 72 wall time consumed: 39.98882985115051\n",
      "batch num: 73 wall time consumed: 40.37295055389404\n",
      "batch num: 74 wall time consumed: 39.69810390472412\n",
      "batch num: 75 wall time consumed: 40.19703269004822\n",
      "batch num: 76 wall time consumed: 40.05048322677612\n",
      "batch num: 77 wall time consumed: 40.29378080368042\n",
      "batch num: 78 wall time consumed: 40.02045488357544\n",
      "batch num: 79 wall time consumed: 41.157652139663696\n",
      "batch num: 80 wall time consumed: 39.684431314468384\n",
      "batch num: 81 wall time consumed: 39.717286348342896\n",
      "batch num: 82 wall time consumed: 39.86164832115173\n",
      "batch num: 83 wall time consumed: 40.21325612068176\n",
      "batch num: 84 wall time consumed: 40.22037100791931\n",
      "batch num: 85 wall time consumed: 39.68337345123291\n",
      "batch num: 86 wall time consumed: 40.03053689002991\n",
      "batch num: 87 wall time consumed: 40.17217969894409\n",
      "batch num: 88 wall time consumed: 39.75106716156006\n",
      "batch num: 89 wall time consumed: 39.81740999221802\n",
      "batch num: 90 wall time consumed: 40.09240770339966\n",
      "batch num: 91 wall time consumed: 39.87443923950195\n",
      "batch num: 92 wall time consumed: 39.76974630355835\n",
      "batch num: 93 wall time consumed: 39.91161632537842\n",
      "batch num: 94 wall time consumed: 39.9528648853302\n",
      "batch num: 95 wall time consumed: 39.96637034416199\n",
      "batch num: 96 wall time consumed: 39.79718732833862\n",
      "batch num: 97 wall time consumed: 40.227434158325195\n",
      "batch num: 98 wall time consumed: 39.94765782356262\n",
      "batch num: 99 wall time consumed: 40.258211851119995\n",
      "batch num: 100 wall time consumed: 39.804479360580444\n",
      "batch num: 101 wall time consumed: 39.79862070083618\n",
      "batch num: 102 wall time consumed: 40.080618143081665\n",
      "batch num: 103 wall time consumed: 40.11222243309021\n",
      "batch num: 104 wall time consumed: 39.73807454109192\n",
      "batch num: 105 wall time consumed: 39.79391837120056\n",
      "batch num: 106 wall time consumed: 40.10137128829956\n",
      "batch num: 107 wall time consumed: 40.08752751350403\n",
      "batch num: 108 wall time consumed: 40.17952060699463\n",
      "batch num: 109 wall time consumed: 39.9392671585083\n",
      "batch num: 110 wall time consumed: 39.90235996246338\n",
      "batch num: 111 wall time consumed: 40.118677377700806\n",
      "batch num: 112 wall time consumed: 40.059932231903076\n",
      "batch num: 113 wall time consumed: 40.03299021720886\n",
      "batch num: 114 wall time consumed: 40.237268686294556\n",
      "batch num: 115 wall time consumed: 40.262397050857544\n",
      "batch num: 116 wall time consumed: 40.16004204750061\n",
      "batch num: 117 wall time consumed: 39.99065709114075\n",
      "batch num: 118 wall time consumed: 39.77810502052307\n",
      "batch num: 119 wall time consumed: 40.03807067871094\n",
      "batch num: 120 wall time consumed: 39.98832440376282\n",
      "batch num: 121 wall time consumed: 39.28131556510925\n",
      "batch num: 122 wall time consumed: 39.37433624267578\n",
      "batch num: 123 wall time consumed: 38.951510190963745\n",
      "batch num: 124 wall time consumed: 39.33266592025757\n",
      "batch num: 125 wall time consumed: 38.86020088195801\n",
      "batch num: 126 wall time consumed: 39.24878764152527\n",
      "batch num: 127 wall time consumed: 39.35402011871338\n",
      "batch num: 128 wall time consumed: 39.00271916389465\n",
      "batch num: 129 wall time consumed: 39.24384522438049\n",
      "batch num: 130 wall time consumed: 39.21375608444214\n",
      "batch num: 131 wall time consumed: 38.98906326293945\n",
      "batch num: 132 wall time consumed: 39.283729791641235\n",
      "batch num: 133 wall time consumed: 39.33757424354553\n",
      "batch num: 134 wall time consumed: 39.18048429489136\n",
      "batch num: 135 wall time consumed: 39.28144407272339\n",
      "batch num: 136 wall time consumed: 39.12085843086243\n",
      "batch num: 137 wall time consumed: 39.13159155845642\n",
      "batch num: 138 wall time consumed: 39.38188409805298\n",
      "batch num: 139 wall time consumed: 39.20531463623047\n",
      "batch num: 140 wall time consumed: 39.19406533241272\n",
      "batch num: 141 wall time consumed: 39.300915479660034\n",
      "batch num: 142 wall time consumed: 39.288023948669434\n",
      "batch num: 143 wall time consumed: 39.414305210113525\n",
      "batch num: 144 wall time consumed: 39.35805058479309\n",
      "batch num: 145 wall time consumed: 39.50939178466797\n",
      "batch num: 146 wall time consumed: 39.003331899642944\n",
      "batch num: 147 wall time consumed: 39.156272411346436\n",
      "batch num: 148 wall time consumed: 39.64461612701416\n",
      "batch num: 149 wall time consumed: 39.037638664245605\n",
      "batch num: 150 wall time consumed: 39.3753604888916\n",
      "batch num: 151 wall time consumed: 39.142780780792236\n",
      "batch num: 152 wall time consumed: 39.38616895675659\n",
      "batch num: 153 wall time consumed: 39.36794400215149\n",
      "batch num: 154 wall time consumed: 39.10295057296753\n",
      "batch num: 155 wall time consumed: 39.260515451431274\n",
      "batch num: 156 wall time consumed: 39.134814977645874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch num: 157 wall time consumed: 39.351991415023804\n",
      "batch num: 158 wall time consumed: 39.3531379699707\n",
      "batch num: 159 wall time consumed: 39.141111612319946\n",
      "batch num: 160 wall time consumed: 39.34543538093567\n",
      "batch num: 161 wall time consumed: 38.92388987541199\n",
      "batch num: 162 wall time consumed: 39.067177295684814\n",
      "batch num: 163 wall time consumed: 39.304216623306274\n",
      "batch num: 164 wall time consumed: 39.302462100982666\n",
      "batch num: 165 wall time consumed: 39.244834423065186\n",
      "batch num: 166 wall time consumed: 38.98465609550476\n",
      "batch num: 167 wall time consumed: 39.18151664733887\n",
      "batch num: 168 wall time consumed: 39.60953235626221\n",
      "batch num: 169 wall time consumed: 39.37199854850769\n",
      "batch num: 170 wall time consumed: 39.077603340148926\n",
      "batch num: 171 wall time consumed: 39.33176231384277\n",
      "batch num: 172 wall time consumed: 40.10268592834473\n",
      "batch num: 173 wall time consumed: 40.035783529281616\n",
      "batch num: 174 wall time consumed: 39.42797064781189\n",
      "batch num: 175 wall time consumed: 39.063124656677246\n",
      "batch num: 176 wall time consumed: 39.192124128341675\n",
      "batch num: 177 wall time consumed: 39.024279832839966\n",
      "batch num: 178 wall time consumed: 39.47062277793884\n",
      "batch num: 179 wall time consumed: 38.822701930999756\n",
      "batch num: 180 wall time consumed: 39.079484701156616\n",
      "batch num: 181 wall time consumed: 38.996243476867676\n",
      "batch num: 182 wall time consumed: 39.16581726074219\n",
      "batch num: 183 wall time consumed: 39.196500301361084\n",
      "batch num: 184 wall time consumed: 39.28188896179199\n",
      "batch num: 185 wall time consumed: 39.3962082862854\n",
      "batch num: 186 wall time consumed: 39.521483182907104\n",
      "batch num: 187 wall time consumed: 39.6782808303833\n",
      "batch num: 188 wall time consumed: 38.60931372642517\n",
      "batch num: 189 wall time consumed: 38.99971413612366\n",
      "batch num: 190 wall time consumed: 39.0612735748291\n",
      "batch num: 191 wall time consumed: 39.129228353500366\n",
      "batch num: 192 wall time consumed: 38.896583795547485\n",
      "batch num: 193 wall time consumed: 39.02157163619995\n",
      "batch num: 194 wall time consumed: 38.90823245048523\n",
      "batch num: 195 wall time consumed: 38.83681321144104\n",
      "batch num: 196 wall time consumed: 39.12993788719177\n",
      "batch num: 197 wall time consumed: 38.683934450149536\n",
      "batch num: 198 wall time consumed: 38.690322160720825\n",
      "batch num: 199 wall time consumed: 38.74775815010071\n",
      "batch num: 200 wall time consumed: 38.80094003677368\n",
      "batch num: 201 wall time consumed: 39.00713777542114\n",
      "batch num: 202 wall time consumed: 39.0112841129303\n",
      "batch num: 203 wall time consumed: 38.74506950378418\n",
      "batch num: 204 wall time consumed: 39.16440176963806\n",
      "batch num: 205 wall time consumed: 38.99947905540466\n",
      "batch num: 206 wall time consumed: 38.92223048210144\n",
      "batch num: 207 wall time consumed: 38.3818039894104\n",
      "batch num: 208 wall time consumed: 38.921343088150024\n",
      "batch num: 209 wall time consumed: 38.74374437332153\n",
      "batch num: 210 wall time consumed: 39.059189796447754\n",
      "batch num: 211 wall time consumed: 38.74126744270325\n",
      "batch num: 212 wall time consumed: 38.987844467163086\n",
      "batch num: 213 wall time consumed: 38.803757429122925\n",
      "batch num: 214 wall time consumed: 39.66826391220093\n",
      "batch num: 215 wall time consumed: 39.1851122379303\n",
      "batch num: 216 wall time consumed: 39.09485650062561\n",
      "batch num: 217 wall time consumed: 39.23890209197998\n",
      "batch num: 218 wall time consumed: 39.24876546859741\n",
      "batch num: 219 wall time consumed: 39.32470726966858\n",
      "batch num: 220 wall time consumed: 39.510215282440186\n",
      "batch num: 221 wall time consumed: 39.23823928833008\n",
      "batch num: 222 wall time consumed: 38.89962339401245\n",
      "batch num: 223 wall time consumed: 39.231263399124146\n",
      "batch num: 224 wall time consumed: 38.753397941589355\n",
      "batch num: 225 wall time consumed: 39.08018517494202\n",
      "batch num: 226 wall time consumed: 39.56419777870178\n",
      "batch num: 227 wall time consumed: 39.57298183441162\n",
      "batch num: 228 wall time consumed: 39.23456406593323\n",
      "batch num: 229 wall time consumed: 38.69581627845764\n",
      "batch num: 230 wall time consumed: 38.920005559921265\n",
      "batch num: 231 wall time consumed: 38.72017312049866\n",
      "batch num: 232 wall time consumed: 38.3829665184021\n",
      "batch num: 233 wall time consumed: 38.48097848892212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RL.dqn.load_nwk_param(run_dir+'/'+ result_type) \n",
    "with open(run_dir+'/hp.pkl','rb') as f:\n",
    "          this_hp = pickle.load(f)\n",
    "          hp.fading_mem = this_hp.fading_mem +0.0 #to avoid assignment by address\n",
    "size=(28,28)\n",
    "offset=(0,0)\n",
    "action_records=[]\n",
    "q_records=[]\n",
    "observation_feeder=np.zeros([batch_size,1024])\n",
    "for image_num,image in enumerate(images):\n",
    "    step = 0\n",
    "    episode = 0\n",
    "for batch_num in range(len(images)//batch_size):\n",
    "    for bb in range(batch_size):\n",
    "        action_list_bb[bb] = []\n",
    "#         q_list_bb[bb] = []\n",
    "        observation_bb[bb] = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n",
    "        observation_bb[bb] = np.random.uniform(0,1,size=[hp.mem_depth, observation_size])\n",
    "#         scene_bb[bb].current_frame = image_num[bb]\n",
    "    \n",
    "    #### sizing story:\n",
    "        image_resized=cv2.resize(0.0+np.reshape(images[batch_num*batch_size+bb],[28,28]), dsize=size)\n",
    "        scene_bb[bb].image = build_mnist_padded([image_resized],y_size=size[1],x_size=size[0],offset=offset)\n",
    "#         scene_bb[bb].image = build_mnist_padded([images[batch_num*batch_size+bb]])\n",
    "\n",
    "        agent_bb[bb].reset()\n",
    "        agent_bb[bb].q_ana[1]=128./2.-32\n",
    "        agent_bb[bb].q_ana[0]=128./2-32\n",
    "        agent_bb[bb].q = np.int32(np.floor(agent_bb[bb].q_ana))\n",
    "\n",
    "        sensor_bb[bb].reset()\n",
    "        sensor_bb[bb].update(scene_bb[bb], agent_bb[bb])\n",
    "        sensor_bb[bb].update(scene_bb[bb], agent_bb[bb])\n",
    "        \n",
    "    time1=time.time()\n",
    "    for step_prime in range(1000):\n",
    "        deep_time1=time.time()\n",
    "#         action = RL.choose_action(observation.reshape([-1]))\n",
    "        for bb in range(batch_size):\n",
    "            observation_feeder[bb,:]=observation_bb[bb].reshape([1,-1])\n",
    "        oo = RL.dqn.eval_eval(observation_feeder)        \n",
    "        boltzmann_measure = np.exp(RL.beta * (oo-np.max(oo,axis=1).reshape([-1,1]))) #todo here substracted max to avoid exponent exploding. need to be taken into a separate function!\n",
    "        boltzmann_measure = boltzmann_measure / np.sum(boltzmann_measure, axis=1).reshape([-1,1])\n",
    "        for bb in range(batch_size):\n",
    "            action_bb[bb] = np.random.choice(list(range(RL.n_actions)),1, p=boltzmann_measure[bb,:].reshape([-1]))[0]\n",
    "        \n",
    "#         action_bb= [a for a in np.argmax(oo,axis=1)]\n",
    "        deep_time2=time.time()\n",
    "        shallow_time1=time.time()\n",
    "        \n",
    "        for bb in range(batch_size):\n",
    "            agent_bb[bb].act(action_bb[bb])\n",
    "            action_list_bb[bb].append(action_bb[bb])\n",
    "#             q_list_bb[bb].append(agent_bb[bb].q_ana)\n",
    "            sensor_bb[bb].update(scene_bb[bb],agent_bb[bb])\n",
    "            observation_bb[bb] *= hp.fading_mem\n",
    "            observation_bb[bb] += local_observer(sensor_bb[bb], agent_bb[bb])  # todo: generalize\n",
    "        shallow_time2=time.time()\n",
    "#         print('deep:',deep_time2-deep_time1,'shallow:',shallow_time2-shallow_time1)\n",
    "    time2=time.time()\n",
    "    \n",
    "    print('batch num:',batch_num,'wall time consumed:',time2-time1)\n",
    "    for bb in range(batch_size):\n",
    "        action_records.append(action_list_bb[bb])\n",
    "#         q_records.append(q_list_bb[bb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59904"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(action_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist_padded_b0p1_v0_X28_Tx0y0_act_full1.pkl','wb') as f:\n",
    "    pickle.dump([action_records[:30000],labels[:30000]],f)\n",
    "    \n",
    "with open('mnist_padded_b0p1_v0_X28_Tx0y0_act_full2.pkl','wb') as f:\n",
    "    pickle.dump([action_records[30000:],labels[30000:]],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sensor_bb[0].frame_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20., 27.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_bb[0].q_ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
