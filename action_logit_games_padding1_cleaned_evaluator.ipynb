{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.classify import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mnist_temp_act_full1.pkl','rb') as f:\n",
    "with open('mnist_padded_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "\n",
    "#mnist_boltz1_act_full1  - gamma=0.99 - beta=0.1 speed penalty =0\n",
    "#mnist_boltz2_act_full1  - gamma=0.99 - beta=1.0 speed penalty =0\n",
    "#mnist_boltz3_act_full1  - gamma=0.99 - beta=1.0 speed penalty =5 \n",
    "#mnist_boltz4_act_full1 - gamma=0.99 - beta=0.03 speed penalty =5\n",
    "#mnist_boltz5_act_full1 - gamma=0.99 - beta=1(orig 0.03) speed penalty =5\n",
    "#6B -  big actions with penalty 200, gamma=0.99 - beta=1.0 (orig 0.03) speed penalty =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "mnist = MNIST('/home/bnapp/datasets/mnist/')\n",
    "\n",
    "_, labels = mnist.load_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = mnist.load_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_n_grams(x,n=None,offsets=None,ngram_vocabulary=None):\n",
    "    if (n is None) and  not (offsets is None):\n",
    "        pass\n",
    "    elif not(n is None) and (offsets is None):\n",
    "        offsets = list(range(n))\n",
    "    else:\n",
    "        error('need to provide either n or offsets')\n",
    "    ngram_dict = {}\n",
    "    for ii in range(len(x)-offsets[-1]):\n",
    "        this_ngram = tuple(x[ii+oo] for oo in offsets)\n",
    "        if (ngram_vocabulary is None) or (this_ngram in ngram_vocabulary):\n",
    "            if this_ngram in ngram_dict.keys():\n",
    "                ngram_dict[this_ngram] +=1\n",
    "            else:\n",
    "                ngram_dict[this_ngram] = 1\n",
    "    return ngram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifiers_temp_n9_l1_sweep_th1000_pp.pkl','rb') as f:\n",
    "    classifiers=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_truncate(ngram_records_trunc,th):\n",
    "    for ii,zz in enumerate(ngram_full):\n",
    "        if ngram_full[zz]<th:\n",
    "            for ng in ngram_records_trunc:\n",
    "                 ng.pop(zz, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets= [0, 2, 3, 4, 6, 8, 9, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max=55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets) for aa in action_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_full={}\n",
    "for rec in ngram_records:\n",
    "    for zz in rec.keys():\n",
    "        if not(zz in ngram_full.keys()):\n",
    "            ngram_full[zz]=rec[zz]\n",
    "        else:\n",
    "            ngram_full[zz]+=rec[zz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Skip from here to your actual action records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_truncate(ngram_records,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8311582381729201\n",
      "0.8829272727272728\n"
     ]
    }
   ],
   "source": [
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print(np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print(np.mean(aatr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing with large (x2) images [original classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mnist_temp_act_full1.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X56_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X56_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets) for aa in action_records]\n",
    "\n",
    "ngram_truncate(ngram_records,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2801794453507341\n",
      "0.27196363636363635\n"
     ]
    }
   ],
   "source": [
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print(np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print(np.mean(aatr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mnist_temp_act_full1.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X42_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X42_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets) for aa in action_records]\n",
    "\n",
    "ngram_truncate(ngram_records,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5316068515497553\n",
      "0.5078181818181818\n"
     ]
    }
   ],
   "source": [
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print(np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print(np.mean(aatr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26386623164763456\n"
     ]
    }
   ],
   "source": [
    "# with open('mnist_temp_act_full1.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X14_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X14_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets) for aa in action_records]\n",
    "\n",
    "ngram_truncate(ngram_records,1000)\n",
    "\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print(np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print(np.mean(aatr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading actions\n",
      "done preparing ngrams\n",
      "done truncating ngrams\n",
      "validation set accuracy: 0.8207585644371941\n",
      "train set accuracy (without retraining): 0.8076\n"
     ]
    }
   ],
   "source": [
    "# with open('mnist_temp_act_full1.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx5y5_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx5y5_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "print('done loading actions')\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets) for aa in action_records]\n",
    "print('done preparing ngrams')\n",
    "ngram_truncate(ngram_records,1000)\n",
    "print('done truncating ngrams')\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preparing ngrams\n",
      "validation set accuracy: 0.8207585644371941\n",
      "train set accuracy (without retraining): 0.8076\n",
      "144.60374689102173\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "\n",
    "ngram_vocabulary= set(classifiers[-1]._vectorizer.feature_names_)\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets,ngram_vocabulary=ngram_vocabulary) for aa in action_records]\n",
    "print('done preparing ngrams')\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx10y10_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx10y10_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "print('done loading actions')\n",
    "\n",
    "ngram_vocabulary= set(classifiers[-1]._vectorizer.feature_names_)\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets,ngram_vocabulary=ngram_vocabulary) for aa in action_records]\n",
    "print('done preparing ngrams')\n",
    "\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))\n",
    "t2=time.time()\n",
    "print('time elapsed:',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set accuracy: 0.8187194127243067\n",
      "train set accuracy (without retraining): 0.8017636363636363\n",
      "time elapsed: 383.3847734928131\n"
     ]
    }
   ],
   "source": [
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))\n",
    "t2=time.time()\n",
    "print('time elapsed:',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading actions\n",
      "done preparing ngrams\n",
      "validation set accuracy: 0.8311582381729201\n",
      "train set accuracy (without retraining): 0.8829272727272728\n",
      "time elapsed: 142.70538687705994\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx0y0_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_b0p1_v5_X28_Tx0y0_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "print('done loading actions')\n",
    "\n",
    "ngram_vocabulary= set(classifiers[-1]._vectorizer.feature_names_)\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets,ngram_vocabulary=ngram_vocabulary) for aa in action_records]\n",
    "print('done preparing ngrams')\n",
    "\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))\n",
    "t2=time.time()\n",
    "print('time elapsed:',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading actions\n",
      "done preparing ngrams\n",
      "validation set accuracy: 0.8311582381729201\n",
      "train set accuracy (without retraining): 0.8829272727272728\n",
      "time elapsed: 148.80495166778564\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "with open('mnist_padded_act_full1.pkl','rb') as f:\n",
    "    action_records1,labels1 = pickle.load(f)\n",
    "# with open('mnist_temp_act_full2.pkl','rb') as f:\n",
    "with open('mnist_padded_act_full2.pkl','rb') as f:\n",
    "    action_records2,labels2 = pickle.load(f)\n",
    "action_records   = action_records1 + action_records2\n",
    "print('done loading actions')\n",
    "\n",
    "ngram_vocabulary= set(classifiers[-1]._vectorizer.feature_names_)\n",
    "ngram_records=[prep_n_grams(aa[:1000],offsets=offsets,ngram_vocabulary=ngram_vocabulary) for aa in action_records]\n",
    "print('done preparing ngrams')\n",
    "\n",
    "val_labels4=classifiers[-1].classify_many(ngram_records[train_max:])\n",
    "aa=[x==y for x,y in zip(val_labels4,labels[train_max:])]\n",
    "print('validation set accuracy:',np.mean(aa))\n",
    "# accu.append(np.mean(aa))\n",
    "\n",
    "train_labels4=classifiers[-1].classify_many(ngram_records[:train_max])\n",
    "aatr=[x==y for x,y in zip(train_labels4,labels[:train_max])]\n",
    "print('train set accuracy (without retraining):',np.mean(aatr))\n",
    "t2=time.time()\n",
    "print('time elapsed:',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True, True, True, True, True, True, True, True, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aatr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remark (applying set to list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019610166549682617\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "vv=set(classifiers[-1]._vectorizer.feature_names_)\n",
    "for ii in range(10000):\n",
    "#     zz=(1, 1, 1, 1, 4, 4, 4, 4, 5) in classifiers[-1]._vectorizer.feature_names_\n",
    "#     zz=(6, 6, 6, 4, 4, 4, 4, 4, 4) in set(classifiers[-1]._vectorizer.feature_names_)\n",
    "    zz=(6, 6, 6, 4, 4, 4, 4, 4, 4) in vv\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.029150485992432\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "vv=set(classifiers[-1]._vectorizer.feature_names_)\n",
    "for ii in range(10000):\n",
    "#     zz=(1, 1, 1, 1, 4, 4, 4, 4, 5) in classifiers[-1]._vectorizer.feature_names_\n",
    "    zz=(6, 6, 6, 4, 4, 4, 4, 4, 4) in set(classifiers[-1]._vectorizer.feature_names_)\n",
    "#     zz=(6, 6, 6, 4, 4, 4, 4, 4, 4) in vv\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.446268796920776\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "for ii in range(10000):\n",
    "    zz=(6, 6, 6, 4, 4, 4, 4, 4, 4) in classifiers[-1]._vectorizer.feature_names_\n",
    "t2=time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
